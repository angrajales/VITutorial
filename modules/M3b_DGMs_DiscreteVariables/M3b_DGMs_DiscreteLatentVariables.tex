\documentclass[14pt]{beamer}

\usetheme{Montpellier}
\usecolortheme{beaver}

\usepackage{amsmath, amssymb, ../../vimacros, hyperref, tikz}
\usepackage{physics}
\usetikzlibrary{positioning, fit, bayesnet, shapes.misc, patterns}
\usepackage[round]{natbib}

\beamertemplatenavigationsymbolsempty

\hypersetup{breaklinks=true, colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue}

\pgfdeclarepatternformonly{stripes}
{\pgfpointorigin}{\pgfpoint{.4cm}{.4cm}}
{\pgfpoint{.4cm}{.4cm}}
{
	\pgfpathmoveto{\pgfpoint{0cm}{0cm}}
	\pgfpathlineto{\pgfpoint{.4cm}{.4cm}}
	\pgfpathlineto{\pgfpoint{.4cm}{.2cm}}
	\pgfpathlineto{\pgfpoint{.2cm}{0cm}}
	\pgfpathclose
	\pgfusepath{fill}
	\pgfpathmoveto{\pgfpoint{0cm}{0.2cm}}
	\pgfpathlineto{\pgfpoint{0cm}{.4cm}}
	\pgfpathlineto{\pgfpoint{0.2cm}{.4cm}}
	\pgfpathclose
	\pgfusepath{fill}
}

\title{Deep Generative Models:\\
Discrete Latent Variables}
\author{Philip Schulz and Wilker Aziz\\
\url{https://github.com/vitutorial/VITutorial}}
\date{}

\setbeamertemplate{footline}[frame number]

\begin{document}

\begin{frame}
\maketitle
\end{frame}

\begin{frame}{What we know so far}
\begin{itemize}
\item Deep Generative Models are probabilistic models where the parameters of the conditional
distributions are computed by neural networks
\pause
\item Because the $ \ELBO $ cannot be computed exactly, we need to sample latent values
\pause
\item Main problem: the MC estimator is not differentiable
\pause
\item Solution: reparametrisation gradient
\end{itemize}
\end{frame}

\begin{frame}{Reparametrisation Gradient}
\begin{block}{Model Gradient}
\begin{equation*}
\frac{\partial}{\partial \theta}\E[q(z|\lambda)]{\log p(x|z,\theta)} - \frac{\partial}{\partial \theta}\KL{q(z|\lambda)}{p(z|\theta)}
\end{equation*}
\end{block}
\pause
\begin{block}{Inference Network Gradient}
\begin{equation*}
\frac{\partial}{\partial \lambda}\E[q(z|\lambda)]{\log p(x|z,\theta)} - \frac{\partial}{\partial \lambda}\KL{q(z|\lambda)}{p(z|\theta)}
\end{equation*}
\end{block}
\end{frame}

\begin{frame}{Reparametrisation Gradient}
\begin{equation*}
\begin{aligned}
&\frac{\partial}{\partial \lambda}\E[q(z|\lambda)]{\log p(x|z,\theta)} &= \\
\pause
&\frac{\partial}{\partial \lambda}\E[\phi(\epsilon)]{\log p(x|\overbrace{h^{-1}(\epsilon, \lambda)}^{z},\theta)} &= \\
\pause
&\E[\phi(\epsilon)]{\frac{\partial}{\partial z}\log p(x|\overbrace{h^{-1}(\epsilon, \lambda)}^{z},\theta) \times \frac{\partial}{\partial \lambda}
\overbrace{h^{-1}(\epsilon, \lambda)}^{z}} &
\end{aligned}
\end{equation*}
\end{frame}

\begin{frame}
\tableofcontents
\end{frame}

\section{Reparametrisation for Discrete Variables?}
\begin{frame}
\tableofcontents[currentsection]
\end{frame}

\begin{frame}{Reparametrisation}
In order to tranform variables, we need to compute the Jacobian (matrix of partial derivatives).
\begin{equation*}
p(z) = p(\overbrace{h(z)}^{\epsilon})\left|\det J_{h(z)}\right|
\end{equation*}
The Jacobian 
\begin{equation}
	J_{ij} = \pdv{h_i(z)}{z_j}
\end{equation}
is generally not available for discrete variables. 
\end{frame}

%\begin{frame}{Cumulative Distribution Function}
%Insert picture here
%\end{frame}

\begin{frame}{Continuity}
The outcome space of discrete variables is non-continuous. Thus, we cannot take derivatives.
\end{frame}

\section{Revisiting the Inference Gradient}
\begin{frame}
\tableofcontents[currentsection]
\end{frame}

\begin{frame}
\begin{equation*}
\begin{aligned}
\frac{\partial}{\partial \lambda}&\E[q(z|\lambda)]{\log p(x|z,\theta)} = \\
\pause
\frac{\partial}{\partial \lambda}&\sum_{z} q(z|\lambda) \log p(x|z,\theta) = \\
\pause
&\sum_{z}\frac{\partial}{\partial \lambda} q(z|\lambda) \log p(x|z,\theta)
\end{aligned}
\end{equation*}
\end{frame}

\begin{frame}{Back to Basic Calculus}
\begin{equation*}
\dv{\lambda}\log f(\lambda) \pause = \frac{\dv{\lambda}f(\lambda)}{f(\lambda)}
\end{equation*}
\pause
\begin{block}{Consequence}
\begin{equation*}
\dv{\lambda}f(\lambda) = \textcolor{blue}{\dv{\lambda}\log f(\lambda) \times f(\lambda)}
\end{equation*}
\end{block}
\end{frame}

\begin{frame}{Score Function Estimator}
\begin{equation*}
\dv{\lambda}f(\lambda) = \textcolor{blue}{\dv{\lambda}\log f(\lambda) \times f(\lambda)}
\end{equation*}
\pause
Apply this to the ELBO derivative.
\begin{equation*}
\begin{aligned}
&\sum_{z}\textcolor{blue}{\frac{\partial}{\partial \lambda}q(z|\lambda)}\times\log p(x|z,\theta) = \\
\pause
&\sum_{z}\textcolor{blue}{q(z|\lambda)\frac{\partial}{\partial \lambda}\log q(z|\lambda)} \times \log p(x|z,\theta) = \\
\pause
&\E[q(z|\lambda)]{\frac{\partial}{\partial \lambda}\log q(z|\lambda) \times \log p(x|z,\theta)}
\end{aligned}
\end{equation*}
\end{frame}

\begin{frame}{Comparison Between Estimators}
\begin{itemize}
\item Score function gradient
\begin{equation*}
\E[q(z|\lambda)]{\frac{\partial}{\partial \lambda}\log q(z|\lambda) \times \log p(x|z,\theta)}
\end{equation*}
\item Reparametrisation gradient
\begin{equation*}
\E[\phi(\epsilon)]{\frac{\partial}{\partial \lambda} \log p(x|h^{-1}(\epsilon, \lambda),\theta)}
\end{equation*}
\end{itemize}
\end{frame}

\begin{frame}{Example Model}
Let us consider a latent factor model for topic modelling. Each document $ x $ consists of $ n $ i.i.d.
categorical draws from that model. The categorical distribution in turn depends on the binary latent 
factors $ z = (z_{1},\ldots,z_{k}) $ which are also i.i.d.
\begin{equation*}
\begin{aligned}
Z_{j} &\sim \BerDist{\phi} && (1 \leq j \leq k) \\ 
X_{i}|z &\sim \CatDist{g(z)} && (1 \leq i \leq n)
\end{aligned}
\end{equation*} 
Here $ g(\cdot) $ is a function computed by neural network with softmax output.
\end{frame}

\begin{frame}{Example Model}
\begin{figure}
\center
\begin{tikzpicture}
\foreach \x in {1,...,4} {
  \pgfmathtruncatemacro{\y}{\x-1}
  \ifthenelse{\x=1}{\node[obs] (x\x) {$ x_{\x} $}}{\node[obs, right= of x\y] (x\x) {$ x_{\x} $}};
}
\foreach \z in {1,2,3} {
  \node[latent, above right = of x\z] (z\z) {$ z_{\z} $};
  \edge{z\z}{x1,x2,x3,x4};
}
\end{tikzpicture}
\end{figure}
At inference time the latent variables are marginally dependent. For our variational distribution
we are going to assume that they are not (recall: mean field assumption).
\end{frame}

\begin{frame}{Inference Network}
\begin{figure}
\center
\begin{tikzpicture}
\foreach \x in {1,...,4} {
\pgfmathtruncatemacro{\y}{\x-1}
\ifthenelse{\x=1}{\node[obs] (x\x) {$ x_{\x} $}}{\node[obs, right= of x\y] (x\x) {$ x_{\x} $}};
}
\foreach \z in {1,2,3} {
  \node[latent, above right = of x\z] (z\z) {$ z_{\z} $};
  \edge[color=red]{x1,x2,x3,x4}{z\z};
}
\end{tikzpicture}
\end{figure}
The inference network needs to predict $ k $ Bernoulli parameters $ \psi $. Any neural network with
sigmoid output will do that job.
\end{frame}

\begin{frame}{Computation Graph}
\begin{figure}
\center
\begin{tikzpicture}[node distance=1cm]
\node[rectangle, draw, rounded corners, thick] (input) {$x$};
\node[rectangle, draw, rounded corners, thick, above left=of input] (psi) {$ \psi $};
\draw[->, thick] (input) -- (psi) node[midway, above, rotate=145] {$ \lambda $};
\node[draw=orange, thick, rectangle, fit= (input) (psi), rounded corners] {};
\node[below left= of input] (inference) {\textcolor{orange}{inference model}};

\pause
\node[rectangle, draw, fill=blue!20, thick, rounded corners, thick, left=of psi] (kl) {$ \KullbackLeibler $};
\draw[->, thick] (psi) edge (kl);

\pause
\node[rectangle, fill=blue!20, thick, above of= psi, rounded corners, draw, node distance=2cm] (scorefunction) {$ \log q(z|\psi) \textcolor{red}{\log p(x|z,\theta)} $};
\draw[->, thick] (psi) edge (scorefunction);

\pause
\node[circle, draw, thick, right= of scorefunction] (z) {$ z $};
\node[rectangle, fill=blue!20, thick, below = of z, rounded corners, draw] (loss) {$ x $};
\node[draw=blue!50, thick, rectangle, fit= (z) (loss), rounded corners] {};
\node[below right= of input] (generation) {\textcolor{blue!50}{generation model}};
\node[right = of z, xshift=-1cm] (random) {$ \sim \BerDist{\psi} $};
\draw[->, thick] (z) -- (loss) node[midway, above, rotate=225] {$ \theta $};
\draw[->, thick] (z) edge (scorefunction);
\draw[->, thick] (loss) edge node[strike out,draw,-,red,double]{} (scorefunction);
\end{tikzpicture}
\end{figure}
\end{frame}

\begin{frame}{Reparametrisation Gradient}
\begin{figure}
\center
\begin{tikzpicture}[node distance=1cm]
\node[rectangle, draw, rounded corners, thick] (input) {$x$};
\node[rectangle, draw, rounded corners, thick, above left=of input] (mu) {$ \mu $};
\node[rectangle, draw, rounded corners, thick, above right=of input] (var) {$ \sigma $};
\node[rectangle, draw, rounded corners, thick, above right= of mu] (z) {$ z $};
\node[rectangle, fill=blue!20, thick, above of= z, rounded corners, draw, node distance=1.5cm] (output) {$ x $};

\draw[->, thick] (input) -- (mu) node[midway, above, rotate=315] {$ \lambda $};
\draw[->, thick] (input) -- (var) node[midway, above, rotate=45] {$ \lambda $};
\draw[->, thick] (mu) edge (z);
\draw[->, thick] (var) edge (z);
\draw[->, thick] (z) -- (output) node[midway, right] {$ \theta $};

\node[draw=orange, thick, rectangle, fit= (input) (mu) (var), rounded corners] {};
\node[left= of mu] (inference) {\textcolor{orange}{inference model}};

\node[draw=blue!50, thick, rectangle, fit= (z) (output), rounded corners] {};
\node[above= of inference] (generation) {\textcolor{blue!50}{generation model}};

\node[circle, draw, thick ,right =of var, xshift=-.5cm] (epsilon) {$ \epsilon $};
\node[right = of epsilon, xshift=-1cm] (stdNormal) {$ \sim \NDist{0}{1} $};
\draw[->, thick] (epsilon) edge (var);

\node[above= of mu, rectangle, draw, fill=blue!20, thick, rounded corners, thick] (KLmu) {$ \KullbackLeibler $};
\draw[->, thick] (mu) edge (KLmu);
\node[above= of var, rectangle, draw, fill=blue!20, thick, rounded corners, thick] (KLvar) {$ \KullbackLeibler $};
\draw[->, thick] (var) edge (KLvar);
\end{tikzpicture}
\end{figure}
\end{frame}

\begin{frame}{Pros and Cons}
\begin{itemize}
\item Pros
\begin{itemize}
\item Applicable to all distributions
\item Many libraries come with samplers for common distributions
\end{itemize}
\pause
\item Cons
\begin{itemize}
\item High Variance!
\end{itemize}
\end{itemize}
\end{frame}

\section{Control Variates and Baselines}

\begin{frame}
\tableofcontents[currentsection]
\end{frame}

\begin{frame}{Baselines}
\begin{block}{Fact}
The Expectation of the score function is 0. 
\pause
\begin{equation*}
\E[q(z|x,\lambda)]{\pdv{\lambda} \log q(z|x,\lambda)} = 0
\end{equation*}
\end{block}
\end{frame}

\begin{frame}{Baselines}
We attempt to centre the gradient estimate. To do this we learn a quantity $ C $ that we subtract
from the reconstruction loss.
\begin{equation*}
\E[q(z|\lambda) ]{\log q(z|\lambda) \left( \log p(x|z,\theta) - C \right)}
\end{equation*}
We call $ C $ a baseline. It does not change the expected gradient \citep{Williams:1992}.
\end{frame}

\begin{frame}{Baselines}
 \begin{equation*}
\begin{aligned}
&\E[q(z|\lambda)]{\pdv{\lambda}\log q(z|\lambda) \left( \log p(x|z,\theta) - C \right)} = \pause \\
&\underbrace{\E[q(z|\lambda)]{\pdv{\lambda}\log q(z|\lambda)  \log p(x|z,\theta)}}_{\text{score function gradient}}  -
\pause \\
&\underbrace{\E[q(z|\lambda)]{\pdv{\lambda}\log q(z|\lambda)}}_{0}C
\end{aligned}
\end{equation*}
\end{frame}

\begin{frame}{Baselines}
We can make baselines input-dependent to make them more flexible.
\begin{equation*}
\log q(z|\lambda) \left( \log p(x|z,\theta) - C(x; \omega) \right)
\end{equation*}

\pause

However, baselines may not depend on the random value $ z $! Quantities that may depend on the
random value ($ C(z) $) are called \textbf{control variates}. \\

~

See \cite{PaisleyEtAl:2012, RanganathEtAl:2014,GregorEtAl:2014}.
\end{frame}

\begin{frame}{Baselines}
Baselines are predicted by a regression model (e.g. a neural net). \\

~

The model is trained using 
an $ L_{2} $-loss.
\begin{equation*}
\min_\omega \left(C(x; \omega) - \log p(x|z,\theta)\right)^{2}
\end{equation*}
\end{frame}

\begin{frame}{Summary}
\begin{itemize}
\pause
\item Reparametrisation not available for discrete variables.
\pause
\item Use score function estimator.
\pause
\item High variance.
\pause
\item Always use baselines for variance reduction!
\end{itemize}
\end{frame}

\section{Semisupervised Learning}

\begin{frame}{Putting it all together}
We now know how to handle continuous and discrete latent variables. Let us combine these two treat partially observed data.
\pause
\begin{block}{Morphological Reinflection}
Transform an inflected form of a verb into another.
\begin{itemize}
\pause
\item plays $ \rightarrow $ played
\pause
\item walking $ \rightarrow $ walks
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{A Simple Model \citep{ZhouNeubig:2017}}
What do we need to correctly inflect a word?
\pause
\begin{itemize}
\item lemma \pause (real vector)
\pause
\item morphological information \pause (discrete vector)
\end{itemize}
\end{frame}

\begin{frame}{A Simple Model \citep{ZhouNeubig:2017}}
\begin{figure}
\center
\begin{tikzpicture}
\node[obs] (input) {$t$};
\node[latent, above left = of input] (lemma) {$ z $};
\node[circle, above right = of input, pattern=stripes, draw=black, pattern color=gray!25, radius=1cm] (morph)  {$ y $};
\node[obs, left = of lemma] (base) {$ s $};
\edge{lemma, morph}{input};
\end{tikzpicture}
\end{figure}
\begin{itemize}
\item $ z $ = lemma (continuous)
\item $ y $ = morphological features (discrete)
\item $ s $ = source form (inflected)
\item $ t $ = target form (inflected)
\end{itemize}
\end{frame}

\begin{frame}{A Simple Model \citep{ZhouNeubig:2017}}
\begin{figure}
\center
\begin{tikzpicture}
\node[obs] (input) {$t$};
\node[latent, above left = of input] (lemma) {$ z $};
\node[circle, above right = of input, pattern=stripes, draw=black, pattern color=gray!25, radius=1cm] (morph)  {$ y $};
\node[obs, left = of lemma] (base) {$ s $};
\edge{lemma, morph}{input};
\path (input) edge[bend right=45, color=red, ->, thick] (morph);
\path (base) edge[color=red, ->, thick] (lemma);
\end{tikzpicture}
\end{figure}
\begin{itemize}
\item $ z $ = lemma (continuous)
\item $ y $ = morphological features (discrete)
\item $ s $ = source form (inflected)
\item $ t $ = target form (inflected)
\end{itemize}
\end{frame}

\begin{frame}{A Simple Model \citep{ZhouNeubig:2017}}
\begin{figure}
\center
\begin{tikzpicture}
\node[obs] (input) {$t$};
\node[latent, above left = of input] (lemma) {$ z $};
\node[circle, above right = of input, pattern=stripes, draw=black, pattern color=gray!25, radius=1cm] (morph)  {$ y $};
\node[obs, left = of lemma] (base) {$ s $};
\edge{lemma, morph}{input};
\path (input) edge[bend right=45, color=red, ->, thick] (morph);
\path (base) edge[color=red, ->, thick] (lemma);
\path (lemma) edge[color=green, ->, thick] (morph);
\end{tikzpicture}
\end{figure}
\begin{itemize}
\item $ z $ = lemma (continuous)
\item $ y $ = morphological features (discrete)
\item $ s $ = source form (inflected)
\item $ t $ = target form (inflected)
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\bibliographystyle{plainnat}
\bibliography{../../VI}
\end{frame}

\begin{frame}{Goodbye and thank you!}
\begin{block}{What we covered today}
\begin{itemize}
\item Derivation of VI
\item Continuous VAE
\item Discrete VAE
\item Semisupervised Models
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Goodbye and thank you!}
\begin{block}{Next steps}
\begin{itemize}
\item Do the coding tutorial (\href{https://github.com/philschulz/VITutorial/blob/master/code/vae_notebook.ipynb}{\url{https://github.com/philschulz/VITutorial/blob/master/code/vae_notebook.ipynb}}) \pause
\item Follow the repository for updates \pause
\item Come to our talk on Tuesday \\
10:30am, ROOM 203/204 \pause
\item Stay in touch
\begin{itemize}
\item w.aziz@uva.nl
\item phschulz@amazon.com
\end{itemize}
\end{itemize}
\end{block}
\end{frame}

\end{document}